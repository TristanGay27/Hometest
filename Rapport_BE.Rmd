---
title: " Poisson Process Project - Testing for homogeneity of a Poisson process"
author: "Ewan DeQuillacq - Tristan Gay - Clément Gris"
date: "2024"
output: pdf_document
latex_engine: xelatex
---

## Introduction

Poisson processes, named after the mathematician Siméon Denis Poisson, are fundamental models in probability theory, widely used to describe phenomena where events occur randomly over time or space. They are characterized by their ability to model independent occurrences with a well-defined average intensity. These processes have diverse applications, notably in reliability analysis, telecommunications systems, and the modeling of claims in the insurance sector.

In this project, we focus on a specific problem related to homogeneous and inhomogeneous Poisson processes. The homogeneous Poisson process (HPP) assumes a constant event intensity, while the inhomogeneous Poisson process (NHPP) allows the intensity to vary over time or space. A key question is to determine whether the intensity $\lambda(\cdot)$ is constant or increasing, which corresponds to testing the null hypothesis $H_0 : \lambda(\cdot)\ \text{is constant}$ against the alternative hypothesis $H_1 : \lambda(\cdot)\ \text{is increasing}$.

This report focuses on the theoretical understanding and experimental validation of statistical tests designed to assess this homogeneity. In particular, we apply these methods to real-world data from large fire insurance claims in Denmark between 1980 and 1990, to determine whether an HPP or NHPP model is more appropriate.

## I - Definition and construction of tests

\
In the first part of this report, we will focus on the theoretical study of the tests described in the first reference article [1].

We will define and explore both homogeneous and inhomogeneous Poisson processes mathematically, outlining their assumptions and fundamental properties. Additionally, we will demonstrate the mathematical constructions underlying these tests, particularly for the Laplace and Weibull distributions, and examine the hypotheses associated with these models.

#### I.1 - Definitions

\
***Homogeneous Poisson process:***

Let $N = (N_t)_t$ be a counting process representing the total number of events that have occurred up to time $t$. A point process $N$ is a *homogeneous Poisson process* with rate $\lambda > 0$ if:

-   $N_0 = 0$

-   Independent increments: The number of points in disjoint time intervals are independent.

-   Stationary increments: The distribution of the number of points in any time interval depends only on the length of the interval, not its position.

    We denote this process as $N \sim \mathcal{P}(\lambda t)$.

***Inhomogeneous Poisson process:***

In the same way, an *inhomogeneous Poisson process* with intensity function $\lambda$ is defined as follows:

-   $N_0 = 0$

-   The increments are independent.

    We denote this process as $N \sim \mathcal{P}\left( \int_0^t \lambda(x) \, dx \right)$.

#### I.2 - Laplace test

\
- Hypotheses :

We test : $$ H_0 : \lambda(.) \; \text{is constant} \quad \text{versus} \quad H_1 : \lambda(.) \; \text{is increasing}$$

-   Statistic :

The test statistic proposed in the article **[1]** is as follows:

$$\begin{aligned} L = \sum_{i=1}^n \frac{T_i}{T*} = \frac{1}{T*} \sum_{i=1}^n T_i \quad &\text{with } T* \text{ s the last observed time,} \\
&\text{et } T_i \text{ is the } i\text{-th observed time.} \end{aligned}$$

Moreover, under $H_0$ and conditionally on ${N = n}$, based on Property 2.23 from the course on conditional distributions, we have:

$$(T_1, T_2, ..., T_n) | \{n = N\} \sim (U_{(1)},U_{(2)},..., U_{(n)}, )$$ $$\text{where } (U_{1},U_{2},..., U_{n}) \text{ are i.i.d. r.v} \sim U([0,T*])$$

Thus, $\frac{T_i}{T^*}$ follows the same distribution as the $i$-th order statistic $U_{(i)}/T^*$, which is a $U([0,1])$ random variable.

$$\text{Reminder : If }X \sim U([0,1]) \, \text{ then } \; \mathbb{E}[X] = \frac{1}{2} \quad \text{ and } \quad Var(X) = \frac{1}{12}$$

By the Central Limit Theorem: $$ \frac{L-\mathbb{E}[L]}{\sqrt{\mathbb{Var}{(L)}}} =  \frac{\sum_{i=1}^n \frac{T_i}{T*} - \frac{n}{2}}{\sqrt{\frac{n}{12}}} \underset {n \rightarrow +\infty} {\overset L \rightarrow} \mathcal{N}(0,1) $$

This is therefore our pivotal statistic.

-   Rejection zone: $H_0$ is rejected when $L$ is large. Indeed, $\lambda(.)$ is constant under $H_0$, so the $T_i/T^*$ are uniformly distributed over $[0,1]$. However, under $H_1$, $\lambda(.)$ is increasing, meaning the $T_i/T^*$ tend to take larger values. Therefore, $L$ will be larger under $H_1$ than under $H_0$. So $$R_{\alpha} = \{ L > l\}$$

-   Error of the first order:

$e_1 = P_{H_0}\left( L > l \right | \{N=n\}) = P_{H_0}\left( \frac{\sum_{i=1}^n \frac{T_i}{T*} - \frac{n}{2}}{\sqrt{\frac{n}{12}}} > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}} | \{N=n\} \right) \underset {n \rightarrow +\infty} {\overset L \rightarrow} P(Z > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}})$

where $Z \sim \mathcal{N}(0,1)$ . And, $$P(Z > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = 1 - P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}})$$

We are conducting a level alpha test:

Then $$ 1 - P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = \alpha <=> P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = 1 - \alpha  $$

So, $\frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}} = z_{1 - \alpha}$ , where $z_{\eta}$ is the $\eta$-quantile of a $\mathcal{N}(0,1)$. Finally, $$l = \sqrt{\frac{n}{12}}z_{1 - \alpha} + \frac{n}{2}$$

-   Conclusion:

The rejection region is given by: $R_{\alpha} = \{L > l\} = \{L > \sqrt{\frac{n}{12}}z_{1 - \alpha} + \frac{n}{2} \} = \{ \frac{L - \frac{n}{2}}{\sqrt{\frac{n}{12}}} > z_{1 - \alpha} \}$

We denote: $X_n = \frac{L - \frac{n}{2}}{\sqrt{\frac{n}{12}}}$ our test statistic.

-   p-value:

The p-value is defined as: $\alpha = P_{H_0}(X_n > X_{n}^{obs}) = 1 - P_{H_0}(X_n < X_{n}^{obs})\underset {n \rightarrow +\infty} {\overset L \rightarrow} 1 - \mathcal{F}_{\mathcal{N}(0,1)}(X_{n}^{obs})$

where $\mathcal{F}_{\mathcal{N}(0,1)}$ is the cumulative distribution function of a distribution $\mathcal{N}(0,1)$.

#### I.3 - Weibull test

\
In this test, we consider $\lambda (t) = (\beta / \theta)(t/ \theta)^{\beta - 1}$ for $\beta, \theta > 0$. We observe that if $\beta = 1$ we recover the exponential distribution with parameter $\lambda(t) = 1/\theta$ corresponding to the simple Poisson process. If $\beta > 1$, the failure rate increases over time (system wear and tear).

-   ​Hypotheses :

We test : $$ H_0 : \beta =1 \quad \text{versus} \quad H_1 : \beta > 1$$

-   Statistic: We denote $Z = 2 \sum_{i=1}^n log(T^* / T_i)$, the test statistic proposed in the article **[1]**.

    Conditionally on $\{N_{T~*} = n\}$, $(T_1, …, T_n)$ behaves as the order statistic associated to n i.i.d. r.v with commun density f:$$f : s \longmapsto \frac{\lambda(s)}{\int_0^{T^*} \lambda(x) dx} \mathbf{1}_{0 < s \le T*} = \frac{\beta}{\theta}(\frac{s}{\theta})^{\beta-1} \frac{1}{\Lambda(T^*)} \mathbf{1}_{0 < s \le T*} =  \frac{\beta s^{\beta - 1}}{\theta^\beta \Lambda(T^*)}  \; \mathbf{1}_{0 < s \le T*}$$

    with $\Lambda(T^*) = \int_0^{T^*} \lambda(x) dx=\int_0^{T^*} \frac{\beta}{\theta}(\frac{x}{\theta})^{\beta-1} dx = \frac{\beta}{\theta^\beta} \int_0^{T*} x^{\beta - 1}dx = \frac{\beta}{\theta^\beta} \bigg[\frac{x^\beta}{\beta}\bigg]_0^{T*} = (\frac{T^*}{\theta})^{\beta}$

Consequently, $f(s) = \frac{\beta s^{\beta - 1}}{\theta^{\beta} \frac{(T^*)^{\beta}}{\theta^{\beta}}} \; \mathbf{1}_{0 < s \le T*} = \frac{\beta s^{\beta - 1}}{T^*} \; \mathbf{1}_{0 < s \le T*}$

To determine the distribution of $Z$, we calculate its moment-generating function.

$$ \mathcal{L}_Z(u) = \mathbb{E}[e^{uZ} | \;\{N_{T^*} = n \}\;]$$

We recall that $$ (T_1, T_2, ..., T_n \;| \; \{N_T = n \}) =^{(d)} (U_{(1)}, U_{(2)}, \; ... \;, U_{(n)})$$

where $U_i$ are **i.i.d.** r.v and of density $f$ defined above.

Therefore, $$\mathcal{L}_Z(u) = \mathbb{E}\left[e^{u \, 2 \sum_{i=1}^n \ln \left(\frac{T^*}{T_i}\right)} \Big| \; \{N_T = n \}\right] \\$$ $$= \mathbb{E}\left[e^{u \, 2 \sum_{i=1}^n \ln \left(\frac{T^*}{U_{(i)}}\right)} \right] \\$$ $$= \mathbb{E}\left[e^{u \, 2 \sum_{i=1}^n \ln \left(\frac{T^*}{U_i}\right)} \right] \\$$ $$= \mathbb{E}\left[ \prod_{i=1}^n e^{u \, 2 \ln \left(\frac{T^*}{U_i}\right)} \right] \\$$ $$= \mathbb{E}\left[ \prod_{i=1}^n \left(\frac{T^*}{U_i}\right)^{2u} \right] \\$$ $$= \left(\mathbb{E}\left[ \left(\frac{T^*}{U_1}\right)^{2u} \right]\right)^n$$

and,

$$  \mathbb{E}\bigg[ (\frac{T*}{U_1})^{2u} \bigg] = \int_{\mathbb{R}} (\frac{T^*}{x})^{2u} \frac{\beta x^{\beta -1}}{T*^{\beta}} \mathbf{1}_{0 < x \le T^*} dx \\
= T^{ * 2u - \beta} \beta \int_0^{T^*} \frac{x^{\beta -1}}{x^{2u}} dx \\ 
= T^{ * 2u - \beta} \beta \bigg[\frac{x^{\beta - 2u}}{\beta -2u} \bigg]_0^{T^*} \\ 
= T^{ * 2u - \beta} \beta \frac{T^{ * 2u - \beta}}{\beta - 2u} \\ 
= \frac{\beta}{\beta - 2u} $$ Then, $(\mathbb{E}\bigg[ (\frac{T*}{U_1})^{2u} \bigg])^n = (\frac{\beta}{\beta - 2u})^n$

Finally, under $H_0$, $\beta = 1$, we have, $\mathcal{L}_Z(u) = (\frac{1}{1 - 2u})^n = (\frac{\frac{1}{2}}{\frac{1}{2} - u})^n$

Here we recognise the generating function of the moments of a Gamma distribution $\Gamma(n, 1/2)$ Moreover, by a property of the course, $\Gamma(n, 1/2) =^{(d)}\chi^2(2n)$ which is the law of Z under $H_0$.

-   Rejection zone: We reject $H_0$ when $Z$ is significantly small, as the reasoning is reversed compared to the previous test due to the use of the reciprocal ratio. Therefore, $$R_{\alpha} = \{ Z < s_{\alpha}\}$$

-   Error of first order:

    $e_1 = P_{H_0}(Z < s_{\alpha} | N = n) = P_{H_0}(2 \sum_{i=1}^n log(T^* / T_i) < s_{\alpha} | N= n) = P(X < s_{\alpha})$ where $X \sim \chi^2(2n)$

    We are conducting a level alpha test:

    Then, $$P(X < s_{\alpha}) \Leftrightarrow \mathbb{P}(X < x_{2n, \alpha}) \le \alpha$$ $\text{and} \quad x_{2n, \alpha} \text{ is the } \alpha -\text{quantile of a } \chi^2(2n)$

-   Conclusion:

    The rejection region is given by: $R_{\alpha}=\{Z < x_{2n, \alpha} \}$ and $Z$ is our test statistic.

-   p-value:

The p-value is defined as: $$\hat{\alpha} = \mathbb{P}_{H_0}(Z<Z^{obs}) = \mathcal{F}_{\chi^2(2n)} (Z^{obs})$$

where $\mathcal{F}_{\chi^2(2n)}$ is the cumulative distribution function of a distribution $\chi^2(2n)$.

## II - Numerical study

\
In this section, we digitally construct the previously developed Laplace and Weibull tests. We analyze the outcomes of these constructions by verifying the alpha levels of the tests and then compare their performance in terms of power.

#### II.1.a - Poisson processes simulation

\
Here, we are building the functions that allow us to construct homogeneous and inhomogeneous Poisson processes.

```{r}
# Homogeneous Poisson process
simulPPh1 <- function(lambda,T_max,fixed_n=-1)
{
  if (fixed_n==-1) fixed_n= rpois(1, lambda*T_max)
  uniformes <- runif(fixed_n, min = 0, max = T_max)
  temps_arrive = sort(uniformes)

  return(temps_arrive)
}
```

```{r}
# Inhomogeneous Poisson process
simulPPi = function(lambda_fct,Tmax,M)
{
  PPh = simulPPh1(M,Tmax)
  n = length(PPh)
  unif = runif(n,0,M)
  val_fct_lamb = lambda_fct(PPh)
  to_return = sort(PPh[unif < val_fct_lamb])
  return(to_return)
}

#Like in the article
simulPPi_asarticle = function(lambda_fct,lambda_inv,T_max,n)
{
  unif = sort(runif(n,0,lambda_fct(T_max)))
  to_return = lambda_inv(unif)
  return(to_return)
}
```

#### II.1.b - Laplace test

\
We define the function that calculates our test statistic and applies the test to the arrival times of a homogeneous or inhomogeneous Poisson process.

```{r}
laplace_stat <- function(arrival_times, T_max) {
  n <- length(arrival_times)
  Y <- sum(arrival_times)/T_max - (n / 2)
  Z <- Y / (sqrt(n / 12))
  return(Z)
}

laplace_test <- function(arrival_times, T_max,alpha){
  X = laplace_stat(arrival_times, T_max)
  pval = 1 - pnorm(X)
  rejete_h0 = (pval < alpha)
  to_return = c(X,pval,rejete_h0)
  return(to_return)
}
```

```{r}
T_max = 10
alpha = 0.05
```

```{r}
PPh1 = simulPPh1(2,T_max)

res = laplace_test(PPh1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this case, for a process with a constant intensity function, we do not reject $H_0$ at the 5% significance level because the p-value is greater than 0.05. This is indeed the expected result.

```{r}
lambda_fct1 <- function(t){return(2*t)}

M1=2*T_max
PPi1 = simulPPi(lambda_fct1,T_max,M1)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

```{r}
lambda_fct_prim <- function(t){return(t^2)}
lambda_inv <- function(t){return(sqrt(t))}

PPi1 = simulPPi_asarticle(lambda_fct_prim,lambda_inv,T_max,40)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this case, we consider an inhomogeneous Poisson process with an increasing intensity function $\lambda(t)= 2t$. We observe a p-value less than 0.05, so we reject $H_0$ at the 5% significance level. This is also the expected result.

The Laplace tests provide consistent results in the trials we conducted. Let's now look at the Weibull tests.

#### II.1.c - Weibull tests

\
Here, we construct our Weibull test statistic from the arrival times as demonstrated in the first part.

```{r}
weibull_stat <- function(arrival_times, T_max) {
  Z <- 2*sum(log(T_max/arrival_times))
  return(Z)
}

weibull_test <- function(arrival_times, T_max,alpha){
  Zobs = weibull_stat(arrival_times, T_max)
  n=length(arrival_times)
  pval = pchisq(Zobs, df = 2 * n)
  rejete_h0 = (pval < alpha)
  to_return = c(Zobs,pval,rejete_h0)
  return(to_return)
}

```

Here, we construct our intensity function dependent on the parameters $\theta$ and $\beta > 0$.

```{r}
lambda_weibull <- function(theta,Beta,t){
  return ((Beta / theta) * (t / theta)^(Beta - 1))
}
```

```{r}
T_max = 10
alpha = 0.05
theta = 1
```

```{r}
lambda_fct1 <- function(t){return(lambda_weibull(theta,1,t))}

M1=lambda_weibull(theta,1,T_max)
PPi1 = simulPPi(lambda_fct1,T_max,M1)

res = weibull_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

```{r}
lambda_fct_prim <- function(t){return(t)}
lambda_inv <- function(t){return(t)}

PPi1 = simulPPi_asarticle(lambda_fct_prim,lambda_inv,T_max,40)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In the case where $\beta = 1$, which corresponds to the null hypothesis $H_0$ of the Weibull test, we obtain a p-value greater than 0.05. Therefore, we do not reject $H_0$ at the 5% significance level. This is consistent with our intensity function, which is constant.

```{r}
b=2
lambda_fct1 <- function(t){return(lambda_weibull(theta,b,t))}

M1=lambda_weibull(theta,b,T_max)
PPi1 = simulPPi(lambda_fct1,T_max,M1)

res = weibull_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

```{r}
lambda_fct_prim <- function(t){return((b/2)*t^2)}
lambda_inv <- function(t){return(sqrt(2*t/b))}

PPi1 = simulPPi_asarticle(lambda_fct_prim,lambda_inv,T_max,40)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this second case, we consider an intensity function with $\beta = 2 > 1$, which is therefore increasing. We obtain a very low p-value and thus reject $H_0$ at the 5% significance level, which is consistent with our hypotheses.

#### II.2 - Comparison of tests

\
In this section, we study the power of the different tests. The function `calculate_power` takes as arguments a test function, the number of simulations to perform in order to estimate the power, and the necessary arguments for the functions performing the test (last arrival time, upper bound of the intensity function, and the intensity function itself).

# INTERPRETATIONS A FAIRE

```{r}
calculate_power <- function(test,n_sim, T_max, lambda_func,lambda_inv, n, alpha = 0.05) {
  rejections <- 0
  ntot = 0
  for (i in 1:n_sim) {
    PPi1 = simulPPi_asarticle(lambda_func,lambda_inv,T_max,n)
    Z <- test(PPi1, T_max,alpha)
    if (!is.nan(Z[3]) && !is.na(Z[3])){
      if (Z[3]) {
        rejections <- rejections + 1
      }
      }
  }
  return(rejections / n_sim)}
```

```{r}
library(ggplot2)
beta_list = c(1.0, 2.0, 4.0)
n_list = c(10, 20, 40)
T_max = 1
n_sim = 5000

for (b in beta_list) {
  powers_laplace = c()
  powers_weibull = c()
  
  for (n in n_list) {
    lambda_func <- function(t) {
      return(t^b) 
    }
    lambda_inv <- function(y){
      return(y^(1/b))
    }
    powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test, n_sim, T_max,lambda_func,lambda_inv, n)
    powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test, n_sim, T_max, lambda_func,lambda_inv, n)
  }
  
  # Convertir en data frame
  df <- data.frame(x = n_list, Laplace = powers_laplace, Weibull = powers_weibull)
  df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                                 names_to = "Distribution", 
                                 values_to = "Power")
  
  # Créer le graphique
  plot <- ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
    geom_point(size = 3) +
    geom_line() +
    labs(title = paste("Power for a Linear Lambda (Beta =", b, ")"),
         x = "Sample Size (n)",
         y = "Power") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red"))  # Personnaliser les couleurs
  
  # Afficher le graphique
  print(plot)
}

```

```{r}
t_list = c(1.0, 2.0, 4.0)
n_list = c(10, 20, 40)
n_sim = 5000

for (T_max in t_list) {
  powers_laplace = c()
  powers_weibull = c()
  
  for (n in n_list) {
    lambda_func <- function(t) {
      return(exp(t)-1) 
    }
    lambda_inv <- function(y){
      return(log(y+1))
    }
    powers_laplace[length(powers_laplace) + 1] <- calculate_power_asarticle(laplace_test, n_sim, T_max,lambda_func,lambda_inv, n)
    powers_weibull[length(powers_weibull) + 1] <- calculate_power_asarticle(weibull_test, n_sim, T_max, lambda_func,lambda_inv, n)
  }
  
  # Convertir en data frame
  df <- data.frame(x = n_list, Laplace = powers_laplace, Weibull = powers_weibull)
  df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                                 names_to = "Distribution", 
                                 values_to = "Power")
  
  # Créer le graphique
  plot <- ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
    geom_point(size = 3) +
    geom_line() +
    labs(title = paste("Power for an exponential Lambda (T_max =", T_max, ")"),
         x = "Sample Size (n)",
         y = "Power") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red"))  # Personnaliser les couleurs
  
  # Afficher le graphique
  print(plot)
}
```

```{r}
to_list = c(1/3, 1/2, 2/3)
n_list = c(10, 20, 40)
n_sim = 1000
T_max=1

for (to in to_list) {
  powers_laplace = c()
  powers_weibull = c()
  
  for (n in n_list) {

    lambda_func <- function(t) {
      return(ifelse(t > to, 2*t - to, t)) 
    }
    lambda_inv <- function(y){
      return(ifelse(y > to,(y + to)/2, y))
    }
    powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test, n_sim, T_max,lambda_func,lambda_inv, n)
    powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test, n_sim, T_max, lambda_func,lambda_inv, n)
  }
  
  # Convertir en data frame
  df <- data.frame(x = n_list, Laplace = powers_laplace, Weibull = powers_weibull)
  df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                                 names_to = "Distribution", 
                                 values_to = "Power")
  
  # Créer le graphique
  plot <- ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
    geom_point(size = 3) +
    geom_line() +
    labs(title = paste("Power for an step function intensity (Tho =", to, ")"),
         x = "Sample Size (n)",
         y = "Power") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red"))  # Personnaliser les couleurs
  
  # Afficher le graphique
  print(plot)
}
```

## III - Application on Danish fires

\
Finally, we apply the homogeneity tests to a real-world dataset consisting of large fire insurance claims in Denmark recorded between 1980 and 1990. This dataset, available in the *evir* R-package under the name "danish," includes the dates of each observation, allowing for a practical evaluation of the tests' performance.

These data represent large fire insurance claims in Denmark, spanning from Thursday, January 3rd, 1980, to Monday, December 31st, 1990. The claims are stored in a numeric vector, with the corresponding dates provided in a `times` attribute, an object of class `POSIXct` (refer to `DateTimeClasses` in R). The dataset was supplied by Mette Rytgaard from Copenhagen Re. It is important to note that these claims constitute an irregular time series.

Why can we consider that fires follow a Poisson process?

We assume that at time $t_0$, no fires have occurred, so $N_0=0$. Furthermore, we hypothesize that the occurrence of one fire does not affect the likelihood of another fire occurring, ensuring that the increments of the process are independent. These assumptions align with the fundamental properties of a Poisson process, making it a suitable model for this phenomenon.

```{r}
library(evir)
library(ggplot2)

data("danish")

times <- attr(danish, "times")
years <- format(times, "%Y")

fire_counts <- table(years)
all_years <- seq(min(as.numeric(names(fire_counts))), max(as.numeric(names(fire_counts))))

fire_data <- data.frame(Year = all_years, 
                        Count = as.numeric(fire_counts[as.character(all_years)]))
fire_data$Count[is.na(fire_data$Count)] <- 0


ggplot(fire_data, aes(x = Year, y = Count)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = Count), vjust = -0.3, size = 3.5) +
  scale_x_continuous(breaks = all_years) + # Afficher toutes les années
  labs(title = "Number of Fire Insurance Claims Over Time",
       x = "Year",
       y = "Number of Claims") 

```

The bar chart shows the annual number of large fire insurance claims in Denmark from 1980 to 1990, highlighting fluctuations over the decade. The number of claims was relatively stable in the early 1980s, ranging from 166 to 181, before dipping to a low of 153 in 1984. This was followed by a sharp increase, peaking at 238 claims in 1986, the highest during the period. The latter half of the decade remained consistently high, with minor fluctuations. Overall, the trend suggests increasing claim activity over time, possibly influenced by external factors such as changing reporting practices, economic conditions, or environmental events.

Let us study the behavior of our intensity function using the Laplace and Weibull tests constructed in the first part. First, we extract the maximum time value from our data. To prevent potential issues in subsequent calculations, we shift each time value by one unit using the following code.

```{r}
times<-attr(danish,"times")
ref<-min(times)
day_ecart<-as.numeric(times-ref,units="days") + 1
day_max<-max(day_ecart) 
```

We now apply our tests to the data we have just retrieved:

```{r}
set.seed(18) 
T_max = day_max
alpha = 0.05
res = laplace_test(day_ecart,T_max,alpha)
print(paste("(Laplace) Rejection of H_0:", as.logical(res[3])))
print(paste("(Laplace) Statistical test observed:",res[1]))
print(paste("(Laplace) p-value:",res[2]))
```

In the context of the Laplace test, we reject $H_0$ at a 5% significance level. This suggests that the intensity function is increasing.

These results provide strong evidence supporting the hypothesis of an increasing intensity function.

```{r}
res = weibull_test(day_ecart,T_max,alpha)
print(paste("(Weibull) Rejection of H_0:", as.logical(res[3])))
print(paste("(Weibull) Statistical test observed:",res[1]))
print(paste("(Weibull) p-value:",res[2]))
```

For the Weibull test, we also reject $H_0$ at a 5% significance level with certainty. Therefore, the intensity function can be assumed to be increasing.

In conclusion, based on the Danish fires dataset, the results of the Laplace and Weibull tests consistently indicate that the intensity function is increasing. This suggests a growing frequency of large fire insurance claims over the observed period. These findings highlight the non-homogeneous nature of the underlying process and demonstrate the practical utility of the proposed tests in real-world applications.

## Conclusion

####  *References* [1] Lee J Bain, Max Engelhardt, and FT Wright. "Tests for an increasing trend in the intensity of a Poisson process: A power study*". Journal of the American Statistical Association*, 80(390):419--422, 1985.
