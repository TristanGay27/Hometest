---
title: "Testing for homogeneity of a Poisson process"
author: "Ewan DeQuillacq - Tristan Gay - Clément Gris"
date: "2024"
output: pdf_document
---

## Introduction

Poisson processes, named after the mathematician Siméon Denis Poisson, are fundamental models in probability theory, widely used to describe phenomena where events occur randomly over time or space. They are characterized by their ability to model independent occurrences with a well-defined average intensity. These processes have diverse applications, notably in reliability analysis, telecommunications systems, and the modeling of claims in the insurance sector.

In this project, we focus on a specific problem related to homogeneous and inhomogeneous Poisson processes. The homogeneous Poisson process (HPP) assumes a constant event intensity, while the inhomogeneous Poisson process (NHPP) allows the intensity to vary over time or space. A key question is to determine whether the intensity $\lambda(\cdot)$ is constant or increasing, which corresponds to testing the null hypothesis $H_0 : \lambda(\cdot)\ \text{is constant}$ against the alternative hypothesis $H_1 : \lambda(\cdot)\ \text{is increasing}$.

This report focuses on the theoretical understanding and experimental validation of statistical tests designed to assess this homogeneity. In particular, we apply these methods to real-world data from large fire insurance claims in Denmark between 1980 and 1990, to determine whether an HPP or NHPP model is more appropriate.

## I - Construction of tests

In the first part of this report, we will focus on the theoretical study of tests described in the first reference article: **Lee J Bain, Max Engelhardt, and FT Wright. Tests for an increasing trend in the intensity of a Poisson process: A power study. Journal of the American Statistical Association, 80(390):419--422, 1985**. **[1]**

We will demonstrate the mathematical constructions underlying these tests, highlighting the assumptions and fundamental properties of homogeneous Poisson processes. We will carry out the construction of hypothesis tests for the Laplace and Weibull distributions.

#### **I.1 - Laplace test**

-   Hypotheses :

We test : $$ H_0 : \lambda(.) \; \text{is constant} \quad \text{versus} \quad H_1 : \lambda(.) \; \text{is increasing}$$

-   Statistic :

The test statistic proposed in the article **[1]** is as follows:

$$\begin{aligned} L = \sum_{i=1}^n \frac{T_i}{T*} = \frac{1}{T*} \sum_{i=1}^n T_i \quad &\text{with } T* \text{ s the last observed time,} \\
&\text{et } T_i \text{ is the } i\text{-th observed time.} \end{aligned}$$

Moreover, under $H_0$ and conditionally on ${N = n}$, based on Property 2.23 from the course on conditional distributions, we have:

$$(T_1, T_2, ..., T_n) | \{n = N\} \sim (U_{(1)},U_{(2)},..., U_{(n)}, )$$ $$\text{where } (U_{1},U_{2},..., U_{n}) \text{ are i.i.d. r.v} \sim U([0,T*])$$

Thus, $\frac{T_i}{T^*}$ follows the same distribution as the $i$-th order statistic $U_{(i)}/T^*$, which is a $U([0,1])$ random variable.

$$\text{Reminder : If }X \sim U([0,1]) \, \text{ then } \; \mathbb{E}[X] = \frac{1}{2} \quad \text{ and } \quad Var(X) = \frac{1}{12}$$

By the Central Limit Theorem: $$ \frac{L-\mathbb{E}[L]}{\sqrt{\mathbb{Var}{(L)}}} =  \frac{\sum_{i=1}^n \frac{T_i}{T*} - \frac{n}{2}}{\sqrt{\frac{n}{12}}} \underset {n \rightarrow +\infty} {\overset L \rightarrow} \mathcal{N}(0,1) $$

This is therefore our pivotal statistic.

-   Rejection zone: $H_0$ is rejected when $L$ is large. Indeed, $\lambda(.)$ is constant under $H_0$, so the $T_i/T^*$ are uniformly distributed over $[0,1]$. However, under $H_1$, $\lambda(.)$ is increasing, meaning the $T_i/T^*$ tend to take larger values. Therefore, $L$ will be larger under $H_1$ than under $H_0$. So $$R_{\alpha} = \{ L > l\}$$

-   Error of the first order:

$e_1 = P_{H_0}\left( L > l \right | \{N=n\}) = P_{H_0}\left( \frac{\sum_{i=1}^n \frac{T_i}{T*} - \frac{n}{2}}{\sqrt{\frac{n}{12}}} > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}} \right | \{N=n\}) \underset {n \rightarrow +\infty} {\overset L \rightarrow} P(Z > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}})$

where $Z \sim \mathcal{N}(0,1)$ . And, $$P(Z > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = 1 - P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}})$$

We are conducting a level alpha test:

Then $$ 1 - P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = \alpha <=> P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = 1 - \alpha  $$

So, $\frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}} = z_{1 - \alpha}$ , where $z_{\eta}$ is the $\eta$-quantile of a $\mathcal{N}(0,1)$. Finally, $$l = \sqrt{\frac{n}{12}}z_{1 - \alpha} + \frac{n}{2}$$

-   Conclusion:

The rejection region is given by: $R_{\alpha} = \{L > l\} = \{L > \sqrt{\frac{n}{12}}z_{1 - \alpha} + \frac{n}{2} \} = \{ \frac{L - \frac{n}{2}}{\sqrt{\frac{n}{12}}} > z_{1 - \alpha} \}$

We denote: $X_n = \frac{L - \frac{n}{2}}{\sqrt{\frac{n}{12}}}$ our test statistic.

-   p-value:

The p-value is defined as: $\alpha = P_{H_0}(X_n > X_{n}^{obs}) = 1 - P_{H_0}(X_n < X_{n}^{obs})\underset {n \rightarrow +\infty} {\overset L \rightarrow} 1 - \mathcal{F}_{\mathcal{N}(0,1)}(X_{n}^{obs})$

where $\mathcal{F}_{\mathcal{N}(0,1)}$ is the cumulative distribution function of a distribution $\mathcal{N}(0,1)$.

#### I.2 - Weibull test

In this test, we consider $\lambda (t) = (\beta / \theta)(t/ \theta)^{\beta - 1}$ for $\beta, \theta > 0$. We observe that if $\beta = 1$ we recover the exponential distribution with parameter $\lambda(t) = 1/\theta$ corresponding to the simple Poisson process. If $\beta > 1$, the failure rate increases over time (system wear and tear).

-   ​Hypotheses :

We test : $$ H_0 : \beta =1 \quad \text{versus} \quad H_1 : \beta > 1$$

-   Statistic: We denote $Z = 2 \sum_{i=1}^n log(T^* / T_i)$, the test statistic proposed in the article **[1]**.

    Conditionally on $\{N_{T~*} = n\}$, $(T_1, …, T_n)$ behaves as the order statistic associated to n i.i.d. r.v with commun density f:$$f : s \longmapsto \frac{\lambda(s)}{\int_0^{T^*} \lambda(x) dx} \mathbf{1}_{0 < s \le T*} = \frac{\beta}{\theta}(\frac{s}{\theta})^{\beta-1} \frac{1}{\Lambda(T^*)} \mathbf{1}_{0 < s \le T*} =  \frac{\beta s^{\beta - 1}}{\theta^\beta \Lambda(T^*)}  \; \mathbf{1}_{0 < s \le T*}$$

    with $\Lambda(T^*) = \int_0^{T^*} \lambda(x) dx=\int_0^{T^*} \frac{\beta}{\theta}(\frac{x}{\theta})^{\beta-1} dx = \frac{\beta}{\theta^\beta} \int_0^{T*} x^{\beta - 1}dx = \frac{\beta}{\theta^\beta} \bigg[\frac{x^\beta}{\beta}\bigg]_0^{T*} = (\frac{T^*}{\theta})^{\beta}$

Consequently, $f(s) = \frac{\beta s^{\beta - 1}}{\theta^{\beta} \frac{(T^*)^{\beta}}{\theta^{\beta}}} \; \mathbf{1}_{0 < s \le T*} = \frac{\beta s^{\beta - 1}}{T^*} \; \mathbf{1}_{0 < s \le T*}$

To determine the distribution of $Z$, we calculate its moment-generating function.

$$ \mathcal{L}_Z(u) = \mathbb{E}[e^{uZ} | \;\{N_{T^*} = n \}\;]$$

We recall that $$ (T_1, T_2, ..., T_n \;| \; \{N_T = n \}) =^{(d)} (U_{(1)}, U_{(2)}, \; ... \;, U_{(n)})$$

where $U_i$ are **i.i.d.** r.v and of density $f$ defined above.

Therefore, $$\mathcal{L}_Z(u) = \mathbb{E}\bigg[e^{u \,2 \sum_{i=1}^n ln \big(\frac{T*}{T_i}\big)}\bigg | \;\{N_T = n \}\;] \\ = \mathbb{E}\bigg[e^{u \,2 \sum_{i=1}^n ln \big(\frac{T*}{U_{(i)}}\big)}\bigg ] \\ = \mathbb{E}\bigg[e^{u \,2 \sum_{i=1}^n ln \big(\frac{T*}{U_i}\big)}\bigg ] \\ = \mathbb{E}\bigg[ \prod_{i=1}^n e^{u \,2 ln \big(\frac{T*}{U_i}\big)}\bigg ]\\ = \mathbb{E}\bigg[ \prod_{i=1}^n  (\frac{T*}{U_i})^{2u}\bigg ]\\ = (\mathbb{E}\bigg[ (\frac{T*}{U_1})^{2u} \bigg])^n $$ and,

$$  \mathbb{E}\bigg[ (\frac{T*}{U_1})^{2u} \bigg] = \int_{\mathbb{R}} (\frac{T^*}{x})^{2u} \frac{\beta x^{\beta -1}}{T*^{\beta}} \mathbf{1}_{0 < x \le T^*} dx \\
= T^{ * 2u - \beta} \beta \int_0^{T^*} \frac{x^{\beta -1}}{x^{2u}} dx \\ 
= T^{ * 2u - \beta} \beta \bigg[\frac{x^{\beta - 2u}}{\beta -2u} \bigg]_0^{T^*} \\ 
= T^{ * 2u - \beta} \beta \frac{T^{ * 2u - \beta}}{\beta - 2u} \\ 
= \frac{\beta}{\beta - 2u} $$ Then, $(\mathbb{E}\bigg[ (\frac{T*}{U_1})^{2u} \bigg])^n = (\frac{\beta}{\beta - 2u})^n$

Finally, under $H_0$, $\beta = 1$, we have, $\mathcal{L}_Z(u) = (\frac{1}{1 - 2u})^n = (\frac{\frac{1}{2}}{\frac{1}{2} - u})^n$

Here we recognise the generating function of the moments of a Gamma distribution $\Gamma(n, 1/2)$ Moreover, by a property of the course, $\Gamma(n, 1/2) =^{(d)}\chi^2(2n)$ which is the law of Z under $H_0$.

-   Rejection zone: We reject $H_0$ when $Z$ is significantly small, as the reasoning is reversed compared to the previous test due to the use of the reciprocal ratio. Therefore, $$R_{\alpha} = \{ Z < s_{\alpha}\}$$

-   Error of first order:

    $e_1 = P_{H_0}(Z < s_{\alpha} | N = n) = P_{H_0}(2 \sum_{i=1}^n log(T^* / T_i) < s_{\alpha} | N= n) = P(X < s_{\alpha})$ where $X \sim \chi^2(2n)$

    We are conducting a level alpha test:

    Then, $$P(X < s_{\alpha}) \Leftrightarrow \mathbb{P}(X < x_{2n, \alpha}) \le \alpha$$ $\text{and} \quad x_{2n, \alpha} \text{ is the } \alpha -\text{quantile of a } \chi^2(2n)$

-   Conclusion:

    The rejection region is given by: $R_{\alpha}=\{Z < x_{2n, \alpha} \}$ and $Z$ is our test statistic.

-   p-value:

The p-value is defined as: $$\hat{\alpha} = \mathbb{P}_{H_0}(Z<Z^{obs}) = \mathcal{F}_{\chi^2(2n)} (Z^{obs})$$

where $\mathcal{F}_{\chi^2(2n)}$ is the cumulative distribution function of a distribution $\chi^2(2n)$.

## II - Numerical study

In this section, we digitally construct the previously developed Laplace and Weibull tests. We analyze the outcomes of these constructions by verifying the alpha levels of the tests and then compare their performance in terms of power.

#### II.1.a - Poisson processes simulation

Here, we are building the functions that allow us to construct homogeneous and inhomogeneous Poisson processes.

```{r}
# Homogeneous Poisson process
simulPPh1 <- function(lambda,Tmax)
{
  nmax= rpois(1, lambda*Tmax)
  uniformes <- runif(nmax, min = 0, max = Tmax)
  temps_arrive = sort(uniformes)

  return(temps_arrive)
}
```

```{r}
# Inhomogeneous Poisson process
simulPPi = function(lambda_fct,Tmax,M)
{
  PPh = simulPPh1(M,Tmax)
  n = length(PPh)
  unif = runif(n,0,M)
  val_fct_lamb = lambda_fct(PPh)
  to_return = sort(PPh[unif < val_fct_lamb])
  return(to_return)
}
```

#### II.1.b - Laplace test

We define the function that calculates our test statistic and applies the test to the arrival times of a homogeneous or inhomogeneous Poisson process.

```{r}
laplace_stat <- function(arrival_times, T_max) {
  n <- length(arrival_times)
  Y <- sum(arrival_times)/T_max - (n / 2)
  Z <- Y / (sqrt(n / 12))
  return(Z)
}

laplace_test <- function(arrival_times, T_max,alpha){
  X = laplace_stat(arrival_times, T_max)
  pval = 1 - pnorm(X)
  rejete_h0 = (pval < alpha)
  to_return = c(X,pval,rejete_h0)
  return(to_return)
}
```

```{r}
T_max = 10
alpha = 0.05
```

```{r}
PPh1 = simulPPh1(2,T_max)

res = laplace_test(PPh1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this case, for a process with a constant intensity function, we do not reject $H_0$ at the 5% significance level because the p-value is greater than 0.05. This is indeed the expected result.

```{r}
lambda_fct1 <- function(t){return(2*t)}

M1=2*Tmax
PPi1 = simulPPi(lambda_fct1,Tmax,M1)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this case, we consider an inhomogeneous Poisson process with an increasing intensity function $λ(t)= 2t$. We observe a p-value less than 0.05, so we reject $H_0$ at the 5% significance level. This is also the expected result.

The Laplace tests provide consistent results in the trials we conducted. Let\'s now look at the Weibull tests.

#### II.1.c - Weibull tests

Here, we construct our Weibull test statistic from the arrival times as demonstrated in the first part.

```{r}
weibull_stat <- function(arrival_times, T_max) {
  Z <- 2*sum(log(T_max/arrival_times))
  return(Z)
}

weibull_test <- function(arrival_times, T_max,alpha){
  Zobs = weibull_stat(arrival_times, T_max)
  n=length(arrival_times)
  pval = pchisq(Zobs, df = 2 * n)
  rejete_h0 = (pval < alpha)
  to_return = c(Zobs,pval,rejete_h0)
  return(to_return)
}

```

Here, we construct our intensity function dependent on the parameters $\theta$ and $\beta > 0$.

```{r}
lambda_weibull <- function(theta,Beta,t){
  return ((Beta / theta) * (t / theta)^(Beta - 1))
}
```

```{r}
T_max = 10
alpha = 0.05
theta = 1
```

```{r}
lambda_fct1 <- function(t){return(lambda_weibull(theta,1,t))}

M1=lambda_weibull(theta,1,T_max)
PPi1 = simulPPi(lambda_fct1,Tmax,M1)

res = weibull_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In the case where $\beta = 1$, which corresponds to the null hypothesis $H_0$ of the Weibull test, we obtain a p-value greater than 0.05. Therefore, we do not reject $H_0$ at the 5% significance level. This is consistent with our intensity function, which is constant.

```{r}
b=2
lambda_fct1 <- function(t){return(lambda_weibull(theta,b,t))}

M1=lambda_weibull(theta,b,T_max)
PPi1 = simulPPi(lambda_fct1,Tmax,M1)

res = weibull_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this second case, we consider an intensity function with $\beta = 2 > 1$, which is therefore increasing. We obtain a very low p-value and thus reject $H_0$ at the 5% significance level, which is consistent with our hypotheses.

#### II.2 - Comparison of tests

In this section, we study the power of the different tests. The function `calculate_power` takes as arguments a test function, the number of simulations to perform in order to estimate the power, and the necessary arguments for the functions performing the test (last arrival time, upper bound of the intensity function, and the intensity function itself).

```{r}
calculate_power <- function(test,n_sim, T_max, borne_sup,lambda_func, alpha = 0.05) {
  rejections <- 0
  ntot = 0
  for (i in 1:n_sim) {
    PPi1 = simulPPi(lambda_func,T_max,borne_sup) 
    Z <- test(PPi1, T_max,alpha)
    if (!is.nan(Z[3]) && !is.na(Z[3])){
      if (Z[3]) {
        rejections <- rejections + 1
      }
      }
  }
  return(rejections / n_sim)
}
```

First, we fix $\theta$ at 1 and vary $\beta$. We display the evolution of the power for the Laplace and Weibull tests according to different values of $\beta$.

```{r}
beta_list = c(1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0)
T_max =10
n_sim=10000

powers_laplace = c()
powers_weibull = c()

for (b in beta_list) {
  lambda_fct <- function(t){return(lambda_weibull(1,b,t))}
  M=lambda_fct(Tmax)
  powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test,n_sim, T_max, M, lambda_fct)
  powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test,n_sim, T_max, M, lambda_fct)
}

# Convertir en data frame
df <- data.frame(x = beta_list, Laplace = powers_laplace,Weibull = powers_weibull)
df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                               names_to = "Distribution", 
                               values_to = "Power")

# Créer le graphique
ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
  geom_point(size = 3) +
  geom_line() +
  labs(title = "Power as function of Beta",
       x = "Beta",
       y = "Power") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red"))  
```

We observe that the Weibull test has a power that increases slightly more rapidly than the Laplace test. However, the powers of both tests are similar for the same values of $\beta$.

In the second step, we fix the value of $\beta$ at 2 and vary the value of $\theta$. We choose values of $\theta$ such that the intensity function is equivalent to a linear function.

```{r}
coef_list = c(0.01,0.1,0.2,0.3,0.4,0.5,0.6,1,2)
T_max =10
n_sim=1000

powers_laplace = c()
powers_weibull = c()

for (coeff in coef_list) {
  lambda_fct <- function(t){return(lambda_weibull(sqrt(2 / coeff),2,t))} # équivalent à lambda(t) = coeff * t
  M=lambda_fct(Tmax)
  powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test,n_sim, T_max, M, lambda_fct)
  powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test,n_sim, T_max, M, lambda_fct)
}
library(ggplot2)

# Convertir en data frame
df <- data.frame(x = coef_list, Laplace = powers_laplace,Weibull = powers_weibull)
df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                               names_to = "Distribution", 
                               values_to = "Power")

# Créer le graphique
ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
  geom_point(size = 3) +
  geom_line() +
  labs(title = "Power for a Linear Lambda",
       x = "Linearity Coefficients",
       y = "Power") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red")) 
```

The results obtained are similar in terms of interpretation to the previous ones. Indeed, the power of the Weibull tests is slightly higher for a fixed $\beta$.

Now, we fix $\beta$ at 3 and vary $\theta$ such that the intensity function is quadratic.

```{r}
coef_list = c(0.01,0.02,0.03,0.04,0.05,0.07,0.08,0.1)
T_max =10
n_sim=1000

powers_laplace = c()
powers_weibull = c()

for (coeff in coef_list) {
  lambda_fct <- function(t){return(lambda_weibull((3 / coeff)^(1 / 3),3,t))} # équivalent à lambda(t) = coeff * t²
  M=lambda_fct(Tmax)
  powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test,n_sim, T_max, M, lambda_fct)
  powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test,n_sim, T_max, M, lambda_fct)
}

# Convertir en data frame
df <- data.frame(x = coef_list, Laplace = powers_laplace,Weibull = powers_weibull)
df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                               names_to = "Distribution", 
                               values_to = "Power")

# Créer le graphique
ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
  geom_point(size = 3) +
  geom_line() +
  labs(title = "Power for a t squared lambda",
       x = "Coefficients of the t squared",
       y = "Power") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red"))
```

For this value of $\beta$, the power increases so rapidly that we need to use small values for the coefficient in front of the intensity function. We observe that the powers are very similar, but there is still a slight difference in favor of the Weibull test.

## III - Application on Danish fires

Finally, we apply the homogeneity tests to a real-world dataset consisting of large fire insurance claims in Denmark recorded between 1980 and 1990. This dataset, available in the *evir* R-package under the name "danish," includes the dates of each observation, allowing for a practical evaluation of the tests' performance.

These data represent large fire insurance claims in Denmark, spanning from Thursday, January 3rd, 1980, to Monday, December 31st, 1990. The claims are stored in a numeric vector, with the corresponding dates provided in a `times` attribute, an object of class `POSIXct` (refer to `DateTimeClasses` in R). The dataset was supplied by Mette Rytgaard from Copenhagen Re. It is important to note that these claims constitute an irregular time series.

```{r}
library(evir)
library(ggplot2)

data("danish")

times <- attr(danish, "times")

years <- format(times, "%Y")

fire_counts <- table(years)
fire_data <- data.frame(Year = as.numeric(names(fire_counts)), Count = as.numeric(fire_counts))

ggplot(fire_data, aes(x = Year, y = Count)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = Count), vjust = -0.3, size = 3.5) +
  labs(title = "Number of Fire Insurance Claims Over Time",
       x = "Year",
       y = "Number of Claims")
```

The bar chart shows the annual number of large fire insurance claims in Denmark from 1980 to 1990, highlighting fluctuations over the decade. The number of claims was relatively stable in the early 1980s, ranging from 166 to 181, before dipping to a low of 153 in 1984. This was followed by a sharp increase, peaking at 238 claims in 1986, the highest during the period. The latter half of the decade remained consistently high, with minor fluctuations. Overall, the trend suggests increasing claim activity over time, possibly influenced by external factors such as changing reporting practices, economic conditions, or environmental events.
