---
title: " Poisson Process Project - Testing for homogeneity of a Poisson process"
author: "Ewan DeQuillacq - Tristan Gay - Clément Gris"
date: "2024"
output: pdf_document
latex_engine: xelatex
---

## Introduction

The Poisson process, both homogeneous and inhomogeneous, plays a central role in statistical modeling due to its ability to describe random events occurring over time or space. We note $N \sim P(\int_0^t\lambda(x)dx)$ where $N$ is the Poisson Process and lambda is called the intensity function.

The distinction between homogeneous and inhomogeneous lies in the intensity function: for a homogeneous Poisson process, the intensity function is constant over time or space, reflecting a uniform rate of occurrences. In contrast, the intensity function of an inhomogeneous Poisson process varies, capturing changes in the rate of events. This flexibility makes Poisson processes particularly useful for a wide range of real-world applications, including modeling customer arrival times in a queue, analyzing the occurrences of earthquake, and tracking the arrival times of insurance claim.

Our project builds upon the article "Tests for an Increasing Trend in the Intensity of a Poisson Process: A Power Study"[1]. This study evaluates the hypothesis of a constant intensity function against the alternative of an increasing intensity function. Various statistical tests are introduced, and their performances are compared using power studies.

Despite its valuable contributions, the study has some limitations. For instance, the explanation of the tests is relatively underexplored. This limitation in the study motivates further investigation and refinement of the proposed methods [1].

The aim of our project is to test the hypothesis of a constant intensity function versus the alternative of an increasing intensity function, following the framework laid out in the article.

To achieve this, we construct several statistical tests, including the Laplace test and the Weibull test. We also perform numerical simulations to evaluate the effectiveness of these tests under various conditions.

The structure of this report is as follows: First, we detail the construction of the statistical tests. Next, we compare the power of these tests using numerical simulations. Finally, we apply our tests to real-world data, specifically Danish fire data, to evaluate their performance in a practical context.

## I - Definition and construction of tests

\
In the first part of this report, we will focus on the theoretical study of the tests described in the first reference article [1].

We will define and explore both homogeneous and inhomogeneous Poisson processes mathematically, outlining their assumptions and fundamental properties. Additionally, we will demonstrate the mathematical constructions underlying these tests, particularly for the Laplace and Weibull distributions, and examine the hypotheses associated with these models.

#### I.1 - Definitions

\
***Homogeneous Poisson process:***

Let $N = (N_t)_t$ be a counting process representing the total number of events that have occurred up to time $t$. A point process $N$ is a *homogeneous Poisson process* with rate $\lambda > 0$ if:

-   $N_0 = 0$

-   Independent increments: The number of points in disjoint time intervals are independent.

-   Stationary increments: The distribution of the number of points in any time interval depends only on the length of the interval, not its position.

    We denote this process as $N \sim \mathcal{P}(\lambda t)$.

***Inhomogeneous Poisson process:***

In the same way, an *inhomogeneous Poisson process* with intensity function $\lambda$ is defined as follows:

-   $N_0 = 0$

-   The increments are independent.

    We denote this process as $N \sim \mathcal{P}\left( \int_0^t \lambda(x) \, dx \right)$.

#### I.2 - Laplace test

We test, $$ H_0 : \lambda(.) \; \text{is constant} \quad \text{versus} \quad H_1 : \lambda(.) \; \text{is increasing}$$

-   Test statistic:

The test statistic proposed in the article is as follows [1].

$$\begin{aligned} L = \sum_{i=1}^n \frac{T_i}{T*} = \frac{1}{T*} \sum_{i=1}^n T_i \quad &\text{with } T* \text{ s the last observed time,} \\
&\text{et } T_i \text{ is the } i\text{-th observed time.} \end{aligned}$$

Moreover, under $H_0$ and conditionally on ${N = n}$, based on Property 2.23 from the course on conditional distributions, we have:

$$(T_1, T_2, ..., T_n) | \{n = N\} \sim (U_{(1)},U_{(2)},..., U_{(n)}, )$$ $$\text{where } (U_{1},U_{2},..., U_{n}) \text{ are i.i.d. r.v} \sim U([0,T*])$$

Thus, $\frac{T_i}{T^*}$ follows the same distribution as the $i$-th order statistic $U_{(i)}/T^*$, which is a uniform random variable $U([0,1])$.

$$\text{Reminder : If }X \sim U([0,1]) \, \text{ then } \; \mathbb{E}[X] = \frac{1}{2} \quad \text{ and } \quad Var(X) = \frac{1}{12}$$

By the Central Limit Theorem, $$ \frac{L-\mathbb{E}[L]}{\sqrt{\mathbb{Var}{(L)}}} =  \frac{\sum_{i=1}^n \frac{T_i}{T*} - \frac{n}{2}}{\sqrt{\frac{n}{12}}} \underset {n \rightarrow +\infty} {\overset L \rightarrow} \mathcal{N}(0,1) $$

This is therefore our pivotal statistic.

-   Rejection zone: the hypothesis $H_0$ is rejected when $L$ is large. Indeed, the intensity function $\lambda(.)$ is constant under $H_0$, so the $T_i/T^*$ are uniformly distributed over $[0,1]$. However, under $H_1$, $\lambda(.)$ is increasing, meaning the $T_i/T^*$ tend to take larger values. Therefore, $L$ will be larger under $H_1$ than under $H_0$. So $$R_{\alpha} = \{ L > l\}$$

-   Error of the first order:

$e_1 = P_{H_0}\left( L > l \right | \{N=n\}) = P_{H_0}\left( \frac{\sum_{i=1}^n \frac{T_i}{T*} - \frac{n}{2}}{\sqrt{\frac{n}{12}}} > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}} | \{N=n\} \right) \underset {n \rightarrow +\infty} {\overset L \rightarrow} P(Z > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}})$

where $Z \sim \mathcal{N}(0,1)$ . And, $$P(Z > \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = 1 - P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}})$$

We are conducting a level alpha test.

Then $$ 1 - P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = \alpha <=> P(Z \le \frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}}) = 1 - \alpha  $$

So, we have $\frac{l - \frac{n}{2}}{\sqrt{\frac{n}{12}}} = z_{1 - \alpha}$ , where $z_{\eta}$ is the $\eta$-quantile of a normal distribution $\mathcal{N}(0,1)$. Finally, $$l = \sqrt{\frac{n}{12}}z_{1 - \alpha} + \frac{n}{2}$$

-   Conclusion:

The rejection region is given by $R_{\alpha} = \{L > l\} = \{L > \sqrt{\frac{n}{12}}z_{1 - \alpha} + \frac{n}{2} \} = \{ \frac{L - \frac{n}{2}}{\sqrt{\frac{n}{12}}} > z_{1 - \alpha} \}$

We denote $X_n = \frac{L - \frac{n}{2}}{\sqrt{\frac{n}{12}}}$ our test statistic.

-   p-value:

The p-value is defined as: $\hat{\alpha} = P_{H_0}(X_n > X_{n}^{obs}) = 1 - P_{H_0}(X_n < X_{n}^{obs})\underset {n \rightarrow +\infty} {\overset L \rightarrow} 1 - \mathcal{F}_{\mathcal{N}(0,1)}(X_{n}^{obs})$

where $\mathcal{F}_{\mathcal{N}(0,1)}$ is the cumulative distribution function of a distribution $\mathcal{N}(0,1)$.

#### I.3 - Weibull test

\
In this test, we consider $\lambda (t) = (\beta / \theta)(t/ \theta)^{\beta - 1}$ for $\beta, \theta > 0$. We observe that if $\beta = 1$ we recover the exponential distribution with parameter $\lambda(t) = 1/\theta$ corresponding to the simple Poisson process. If $\beta > 1$, the failure rate increases over time (system wear and tear).

We test $$ H_0 : \beta =1 \quad \text{versus} \quad H_1 : \beta > 1$$

-   Statistic: We denote $Z = 2 \sum_{i=1}^n log(T^* / T_i)$, the test statistic proposed in the article [1].

    Conditionally on $\{N_{T~*} = n\}$, $(T_1, …, T_n)$ behaves as the order statistic associated to n independent and identically distributed random variable with commun density $f$:$$f : s \longmapsto \frac{\lambda(s)}{\int_0^{T^*} \lambda(x) dx} \mathbf{1}_{0 < s \le T*} = \frac{\beta}{\theta}(\frac{s}{\theta})^{\beta-1} \frac{1}{\Lambda(T^*)} \mathbf{1}_{0 < s \le T*} =  \frac{\beta s^{\beta - 1}}{\theta^\beta \Lambda(T^*)}  \; \mathbf{1}_{0 < s \le T*}$$

    with $\Lambda(T^*) = \int_0^{T^*} \lambda(x) dx=\int_0^{T^*} \frac{\beta}{\theta}(\frac{x}{\theta})^{\beta-1} dx = \frac{\beta}{\theta^\beta} \int_0^{T*} x^{\beta - 1}dx = \frac{\beta}{\theta^\beta} \bigg[\frac{x^\beta}{\beta}\bigg]_0^{T*} = (\frac{T^*}{\theta})^{\beta}$

Consequently, $f(s) = \frac{\beta s^{\beta - 1}}{\theta^{\beta} \frac{(T^*)^{\beta}}{\theta^{\beta}}} \; \mathbf{1}_{0 < s \le T*} = \frac{\beta s^{\beta - 1}}{T^*} \; \mathbf{1}_{0 < s \le T*}$

To determine the distribution of $Z$, we calculate its moment-generating function.

$$ \mathcal{L}_Z(u) = \mathbb{E}[e^{uZ} | \;\{N_{T^*} = n \}\;]$$

We recall that $$ (T_1, T_2, ..., T_n \;| \; \{N_T = n \}) =^{(d)} (U_{(1)}, U_{(2)}, \; ... \;, U_{(n)})$$

where $U_i$ areindependent and identically distributed random variable and of density $f$ defined above.

Therefore, $$\mathcal{L}_Z(u) = \mathbb{E}\left[e^{u \, 2 \sum_{i=1}^n \ln \left(\frac{T^*}{T_i}\right)} \Big| \; \{N_T = n \}\right] \\$$ $$= \mathbb{E}\left[e^{u \, 2 \sum_{i=1}^n \ln \left(\frac{T^*}{U_{(i)}}\right)} \right] \\$$ $$= \mathbb{E}\left[e^{u \, 2 \sum_{i=1}^n \ln \left(\frac{T^*}{U_i}\right)} \right] \\$$ $$= \mathbb{E}\left[ \prod_{i=1}^n e^{u \, 2 \ln \left(\frac{T^*}{U_i}\right)} \right] \\$$ $$= \mathbb{E}\left[ \prod_{i=1}^n \left(\frac{T^*}{U_i}\right)^{2u} \right] \\$$ $$= \left(\mathbb{E}\left[ \left(\frac{T^*}{U_1}\right)^{2u} \right]\right)^n$$

and,

$$  \mathbb{E}\bigg[ (\frac{T*}{U_1})^{2u} \bigg] = \int_{\mathbb{R}} (\frac{T^*}{x})^{2u} \frac{\beta x^{\beta -1}}{T*^{\beta}} \mathbf{1}_{0 < x \le T^*} dx \\
= T^{ * 2u - \beta} \beta \int_0^{T^*} \frac{x^{\beta -1}}{x^{2u}} dx \\ 
= T^{ * 2u - \beta} \beta \bigg[\frac{x^{\beta - 2u}}{\beta -2u} \bigg]_0^{T^*} \\ 
= T^{ * 2u - \beta} \beta \frac{T^{ * 2u - \beta}}{\beta - 2u} \\ 
= \frac{\beta}{\beta - 2u} $$ Then, $(\mathbb{E}\bigg[ (\frac{T*}{U_1})^{2u} \bigg])^n = (\frac{\beta}{\beta - 2u})^n$

Finally, under the hypothesis $H_0$, the parameter $\beta = 1$, therefore we have$\mathcal{L}_Z(u) = (\frac{1}{1 - 2u})^n = (\frac{\frac{1}{2}}{\frac{1}{2} - u})^n$

Here we recognise the generating function of the moments of a Gamma distribution $\Gamma(n, 1/2)$.

Moreover, by a property of the course, $\Gamma(n, 1/2) =^{(d)}\chi^2(2n)$ which is the law of $Z$ under $H_0$.

-   Rejection zone: We reject $H_0$ when $Z$ is significantly small, as the reasoning is reversed compared to the previous test due to the use of the reciprocal ratio. Therefore, $$R_{\alpha} = \{ Z < s_{\alpha}\}$$

-   Error of first order:

    $e_1 = P_{H_0}(Z < s_{\alpha} | N = n) = P_{H_0}(2 \sum_{i=1}^n log(T^* / T_i) < s_{\alpha} | N= n) = P(X < s_{\alpha})$ where $X \sim \chi^2(2n)$

    We are conducting a level alpha test:

    Then, $$P(X < s_{\alpha}) \Leftrightarrow \mathbb{P}(X < x_{2n, \alpha}) \le \alpha$$ $\text{and} \quad x_{2n, \alpha} \text{ is the } \alpha -\text{quantile of a } \chi^2(2n)$

-   Conclusion:

    The rejection region is given by: $R_{\alpha}=\{Z < x_{2n, \alpha} \}$ and $Z$ is our test statistic.

-   p-value:

The p-value is defined as: $$\hat{\alpha} = \mathbb{P}_{H_0}(Z<Z^{obs}) = \mathcal{F}_{\chi^2(2n)} (Z^{obs})$$

where $\mathcal{F}_{\chi^2(2n)}$ is the cumulative distribution function of a distribution $\chi^2(2n)$.

## II - Numerical study

In this section, we digitally construct the previously developed Laplace and Weibull tests. We analyze the outcomes of these constructions by verifying the alpha levels of the tests and then compare their performance in terms of power.

#### II.1.a - Poisson processes simulations

Here, we are building the functions that allow us to construct homogeneous and inhomogeneous Poisson processes. We construct inhomogeneous Poisson processes in two ways: as discussed in the course, and as outlined in the article [1].

```{r}
# Homogeneous Poisson process
simulPPh1 <- function(lambda,T_max,fixed_n=-1)
{
  if (fixed_n==-1) fixed_n= rpois(1, lambda*T_max)
  uniformes <- runif(fixed_n, min = 0, max = T_max)
  temps_arrive = sort(uniformes)

  return(temps_arrive)
}
```

```{r}
# Inhomogeneous Poisson process
simulPPi = function(lambda_fct,Tmax,M)
{
  PPh = simulPPh1(M,Tmax)
  n = length(PPh)
  unif = runif(n,0,M)
  val_fct_lamb = lambda_fct(PPh)
  to_return = sort(PPh[unif < val_fct_lamb])
  return(to_return)
}

#Like in the article
simulPPi_asarticle = function(lambda_fct,lambda_inv,T_max,n)
{
  unif = sort(runif(n,0,lambda_fct(T_max)))
  to_return = lambda_inv(unif)
  return(to_return)
}
```

#### II.1.b - Laplace test

We define the function that calculates our test statistic and applies the test to the arrival times of a homogeneous or inhomogeneous Poisson process.

```{r}
laplace_stat <- function(arrival_times, T_max) {
  n <- length(arrival_times)
  Y <- sum(arrival_times)/T_max - (n / 2)
  Z <- Y / (sqrt(n / 12))
  return(Z)
}

laplace_test <- function(arrival_times, T_max,alpha){
  X = laplace_stat(arrival_times, T_max)
  pval = 1 - pnorm(X)
  rejete_h0 = (pval < alpha)
  to_return = c(X,pval,rejete_h0)
  return(to_return)
}
```

```{r}
T_max = 10
alpha = 0.05
```

```{r}
PPh1 = simulPPh1(2,T_max)

res = laplace_test(PPh1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this case, for a process with a constant intensity function, we do not reject the hypothesis $H_0$ at the 5% significance level because the p-value is greater than $0.05$. This is indeed the expected result.

We compare two methods to simulate an inhomogeneous Poisson process: the thinning algorithm and the method described in [1].

```{r}
lambda_fct1 <- function(t){return(2*t)}

M1=2*T_max
PPi1 = simulPPi(lambda_fct1,T_max,M1)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

```{r}
lambda_fct_prim <- function(t){return(t^2)}
lambda_inv <- function(t){return(sqrt(t))}

PPi1 = simulPPi_asarticle(lambda_fct_prim,lambda_inv,T_max,40)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this case, we consider an inhomogeneous Poisson process with an increasing intensity function $\lambda(t)= 2t$. We observe a p-value less than $0.05$, so we reject $H_0$ at the 5% significance level. This is also the expected result.

The Laplace tests provide consistent results in the trials we conducted. Let's now look at the Weibull tests.

#### II.1.c - Weibull tests

Here, we construct our Weibull test statistic from the arrival times as demonstrated in the first part.

```{r}
weibull_stat <- function(arrival_times, T_max) {
  Z <- 2*sum(log(T_max/arrival_times))
  return(Z)
}

weibull_test <- function(arrival_times, T_max,alpha){
  Zobs = weibull_stat(arrival_times, T_max)
  n=length(arrival_times)
  pval = pchisq(Zobs, df = 2 * n)
  rejete_h0 = (pval < alpha)
  to_return = c(Zobs,pval,rejete_h0)
  return(to_return)
}

```

Here, we construct our intensity function dependent on the parameters $\theta$ and $\beta > 0$.

```{r}
lambda_weibull <- function(theta,Beta,t){
  return ((Beta / theta) * (t / theta)^(Beta - 1))
}
```

```{r}
T_max = 10
alpha = 0.05
theta = 1
```

```{r}
lambda_fct1 <- function(t){return(lambda_weibull(theta,1,t))}

M1=lambda_weibull(theta,1,T_max)
PPi1 = simulPPi(lambda_fct1,T_max,M1)

res = weibull_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

```{r}
lambda_fct_prim <- function(t){return(t)}
lambda_inv <- function(t){return(t)}

PPi1 = simulPPi_asarticle(lambda_fct_prim,lambda_inv,T_max,40)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In the case where $\beta = 1$, which corresponds to the null hypothesis $H_0$ of the Weibull test, we obtain a p-value greater than 0.05. Therefore, we do not reject $H_0$ at the 5% significance level. This is consistent with our intensity function, which is constant.

```{r}
b=2
lambda_fct1 <- function(t){return(lambda_weibull(theta,b,t))}

M1=lambda_weibull(theta,b,T_max)
PPi1 = simulPPi(lambda_fct1,T_max,M1)

res = weibull_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

```{r}
lambda_fct_prim <- function(t){return((b/2)*t^2)}
lambda_inv <- function(t){return(sqrt(2*t/b))}

PPi1 = simulPPi_asarticle(lambda_fct_prim,lambda_inv,T_max,40)

res = laplace_test(PPi1,T_max,alpha)
print(paste("Rejection of H_0:", as.logical(res[3])))
print(paste("Statistical test observed:",res[1]))
print(paste("p-value:",res[2]))
```

In this second case, we consider an intensity function with $\beta = 2 > 1$, which is therefore increasing. We obtain a very low p-value and thus reject $H_0$ at the 5% significance level, which is consistent with our hypotheses.

#### II.2 - Comparison of tests

In this section, we study the power of the different tests. The function `calculate_power` takes as arguments a test function, the number of simulations to perform in order to estimate the power, and the necessary arguments for the functions performing the test (last arrival time, upper bound of the intensity function, and the intensity function itself).

```{r}
calculate_power <- function(test,n_sim, T_max, lambda_func,lambda_inv, n, alpha = 0.05) {
  rejections <- 0
  ntot = 0
  for (i in 1:n_sim) {
    PPi1 = simulPPi_asarticle(lambda_func,lambda_inv,T_max,n)
    Z <- test(PPi1, T_max,alpha)
    if (!is.nan(Z[3]) && !is.na(Z[3])){
      if (Z[3]) {
        rejections <- rejections + 1
      }
      }
  }
  return(rejections / n_sim)}
```

We revisit the different cases presented in [1]. First, we study the power when the intensity function is a Weibull function. Next, we examine cases where the intensity function is exponential, logarithmic, and finally, a step function. To do so, as in the article, we fix both $T_{max}$ and the number of occurrences $n$.

```{r}
library(ggplot2)
beta_list = c(1.0, 2.0, 4.0)
n_list = c(10, 20, 40)
T_max = 1
n_sim = 5000

for (b in beta_list) {
  powers_laplace = c()
  powers_weibull = c()
  
  for (n in n_list) {
    lambda_func <- function(t) {
      return(t^b) 
    }
    lambda_inv <- function(y){
      return(y^(1/b))
    }
    powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test, n_sim, T_max,lambda_func,lambda_inv, n)
    powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test, n_sim, T_max, lambda_func,lambda_inv, n)
  }
  
  # Convertir en data frame
  df <- data.frame(x = n_list, Laplace = powers_laplace, Weibull = powers_weibull)
  df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                                 names_to = "Distribution", 
                                 values_to = "Power")
  
  # Créer le graphique
  plot <- ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
    geom_point(size = 3) +
    geom_line() +
    labs(title = paste("Power for a Weibull function (Beta =", b, ")"),
         x = "Sample Size (n)",
         y = "Power") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red"))  # Personnaliser les couleurs
  
  # Afficher le graphique
  print(plot)
}

```

We obtain the same results as in [1]. First, the larger $\beta$, the more the function increases, and the more confidently we reject $H_0$. This result holds when $\beta = 1$, where we are under $H_0$ and the power is approximately $0.05$ for both tests. When $\beta = 2$, the power is about $0.6$ for $n = 10$, rising to $0.98$ for $n = 40$. For $\beta = 4$, the power is almost always equal to $1$ for both tests.

The larger $n$, the greater the power for $\beta = 2$ or $\beta = 4$. We have an asymptotic test statistic in both cases, so the larger $n$ is, the closer we get to its asymptotic distribution. As a result, the more powerful the test.

Finally, we observe that the red curve (Weibull) is always above the blue curve (Laplace). For this type of function, the Weibull test is more powerful and is therefore recommended.

```{r}
t_list = c(1.0, 2.0, 4.0)
n_list = c(10, 20, 40)
n_sim = 5000

for (T_max in t_list) {
  powers_laplace = c()
  powers_weibull = c()
  
  for (n in n_list) {
    lambda_func <- function(t) {
      return(exp(t)-1) 
    }
    lambda_inv <- function(y){
      return(log(y+1))
    }
    powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test, n_sim, T_max,lambda_func,lambda_inv, n)
    powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test, n_sim, T_max, lambda_func,lambda_inv, n)
  }
  
  # Convertir en data frame
  df <- data.frame(x = n_list, Laplace = powers_laplace, Weibull = powers_weibull)
  df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                                 names_to = "Distribution", 
                                 values_to = "Power")
  
  # Créer le graphique
  plot <- ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
    geom_point(size = 3) +
    geom_line() +
    labs(title = paste("Power for an exponential Lambda (T_max =", T_max, ")"),
         x = "Sample Size (n)",
         y = "Power") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red"))  # Personnaliser les couleurs
  
  # Afficher le graphique
  print(plot)
}
```

We observe that as $T_{\text{max}}$ increases, the power also increases. For $n = 40$, the power is around $0.5$ when $T_{\text{max}} = 1$, whereas it reaches $1$ when $T_{\text{max}} = 4$ .

Similarly, as $n$ increases, the power also grows. For instance, for any $T_{\text{max}}$, the curves are increasing.

The larger $T_{\text{max}}$, the more easily the growth of the intensity function becomes observable, as it follows an exponential pattern. Consequently, as $T_{\text{max}}$ increases, the power approaches $1$.

Additionally, the power curve for Laplace is always above that for Weibull. The Laplace test is therefore more powerful for exponential-type functions.

```{r}
t_list = c(10, 15, 25)
n_list = c(10, 20, 40)
n_sim = 5000

for (T_max in t_list) {
  powers_laplace = c()
  powers_weibull = c()
  
  for (n in n_list) {
    lambda_func <- function(t) {
      return((1 + t) * log(1 + t) - t) 
    }
    
    lambda_inv <- function(y) {
        # Fonction d'inversion pour un scalaire
        inverse_scalar <- function(single_y) {
          if (lambda_func(0) > single_y || lambda_func(100) < single_y) {
            stop("The function does not change sign in the interval [0, 100]")
          }
          result <- uniroot(function(t) lambda_func(t) - single_y, lower = 0, upper = 100)
          return(result$root)
        }
        # Appliquer à chaque élément de y si y est un vecteur
        return(sapply(y, inverse_scalar))
      }
    
    
    powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test, n_sim, T_max,lambda_func,lambda_inv, n)
    powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test, n_sim, T_max, lambda_func,lambda_inv, n)
  }
  
  # Convertir en data frame
  df <- data.frame(x = n_list, Laplace = powers_laplace, Weibull = powers_weibull)
  df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                                 names_to = "Distribution", 
                                 values_to = "Power")
  
  # Créer le graphique
  plot <- ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
    geom_point(size = 3) +
    geom_line() +
    labs(title = paste("Power for an log Lambda (T_max =", T_max, ")"),
         x = "Sample Size (n)",
         y = "Power") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red"))  # Personnaliser les couleurs
  
  # Afficher le graphique
  print(plot)
}
```

**Remark:** When integrating the function $\lambda(t) = \log(1 + t)$, the resulting function does not have an explicit inverse. We approximate it using the `uniroot` function in **R**.

As $T_{\text{max}}$ increases, the power decreases. For example, for $n = 40$, the power for the Laplace test is 0.75 when $T_{\text{max}} = 10$ and decreases to $0.6$ when $T_{\text{max}} = 25$ .

Cela vient du fait que la fonction log(1+t) a une croissante de moins en moins importante lorsque t est grand. Ainsi, pour un T_max elevé, la fonction d'intensité peut sembler presque constante car la forte croissance du début est atténuée par la stabilisation de la fonction. C'est pourquoi, plus T_max augmente, plus la puissance de nos tests diminue.

As $n$ increases, the power also increases, for the same reason as mentioned earlier.

The power of the Weibull test is always greater than that of the Laplace test.

```{r}
to_list = c(1/3, 1/2, 2/3)
n_list = c(10, 20, 40)
n_sim = 1000
T_max=1

for (to in to_list) {
  powers_laplace = c()
  powers_weibull = c()
  
  for (n in n_list) {

    lambda_func <- function(t) {
      return(ifelse(t > to, 2*t - to, t)) 
    }
    lambda_inv <- function(y){
      return(ifelse(y > to,(y + to)/2, y))
    }
    powers_laplace[length(powers_laplace) + 1] <- calculate_power(laplace_test, n_sim, T_max,lambda_func,lambda_inv, n)
    powers_weibull[length(powers_weibull) + 1] <- calculate_power(weibull_test, n_sim, T_max, lambda_func,lambda_inv, n)
  }
  
  # Convertir en data frame
  df <- data.frame(x = n_list, Laplace = powers_laplace, Weibull = powers_weibull)
  df_long <- tidyr::pivot_longer(df, cols = c("Laplace", "Weibull"), 
                                 names_to = "Distribution", 
                                 values_to = "Power")
  
  # Créer le graphique
  plot <- ggplot(df_long, aes(x = x, y = Power, color = Distribution, group = Distribution)) +
    geom_point(size = 3) +
    geom_line() +
    labs(title = paste("Power for an step function intensity (Tho =", to, ")"),
         x = "Sample Size (n)",
         y = "Power") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red"))  # Personnaliser les couleurs
  
  # Afficher le graphique
  print(plot)
}
```

As $n$ increases, the power becomes greater for the same reasons as mentioned previously.

For the Weibull test, the power tends to decrease as $\theta$ increases. Lower values are observed for $\theta = 2/3$, while similar values are obtained for $\theta = 1/3$ and $\theta = 1/2$.

In contrast, for the Laplace test, the power appears to slightly increase with $\theta$. Lower values are observed for $\theta = 1/3$, while similar values are obtained for $\theta = 1/3$ and $\theta = 2/3$.

Thus, for $\theta = 1/3$, the Weibull test is more powerful. For the other $\theta$ value studied, the Laplace test is more powerful.

Overall, these simulations confirm the results presented in [1].

## III - Application on Danish fires

\
Finally, we apply the homogeneity tests to a real-world dataset consisting of large fire insurance claims in Denmark recorded between 1980 and 1990. This dataset, available in the *evir* R-package under the name "danish," includes the dates of each observation, allowing for a practical evaluation of the tests' performance.

These data represent large fire insurance claims in Denmark, spanning from Thursday, January 3rd, 1980, to Monday, December 31st, 1990. The claims are stored in a numeric vector, with the corresponding dates provided in a `times` attribute, an object of class `POSIXct` (refer to `DateTimeClasses` in R). The dataset was supplied by Mette Rytgaard from Copenhagen Re. It is important to note that these claims constitute an irregular time series.

Why can we consider that fires follow a Poisson process?

We assume that at time $t_0$, no fires have occurred, so $N_0=0$. Furthermore, we hypothesize that the occurrence of one fire does not affect the likelihood of another fire occurring, ensuring that the increments of the process are independent. These assumptions align with the fundamental properties of a Poisson process, making it a suitable model for this phenomenon.

```{r}
library(evir)
library(ggplot2)

data("danish")
times <- attr(danish, "times")

ref<-min(times)
day_ecart<-as.numeric(times-ref,units="days") + 1
day_max<-max(day_ecart) 

years <- format(times, "%Y")

fire_counts <- table(years)
all_years <- seq(min(as.numeric(names(fire_counts))), max(as.numeric(names(fire_counts))))

fire_data <- data.frame(Year = all_years, 
                        Count = as.numeric(fire_counts[as.character(all_years)]))
fire_data$Count[is.na(fire_data$Count)] <- 0


ggplot(fire_data, aes(x = Year, y = Count)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = Count), vjust = -0.3, size = 3.5) +
  scale_x_continuous(breaks = all_years) + # Afficher toutes les années
  labs(title = "Number of Fire Insurance Claims Over Time",
       x = "Year",
       y = "Number of Claims") 

```

The bar chart shows the annual number of large fire insurance claims in Denmark from 1980 to 1990, highlighting fluctuations over the decade. The number of claims was relatively stable in the early 1980s, ranging from 166 to 181, before dipping to a low of 153 in 1984. This was followed by a sharp increase, peaking at 238 claims in 1986, the highest during the period. The latter half of the decade remained consistently high, with minor fluctuations. Overall, the trend suggests increasing claim activity over time, possibly influenced by external factors such as changing reporting practices, economic conditions, or environmental events.

We aim to study the function $\lambda$ that describes our Poisson process. We consider the four functions previously analyzed in the power tests to determine which one best describes our process.

```{r}
plot.PP<- function(PP)
{
  plot(c(0,PP),0:length(PP),type="s",xlab="time t",ylab="number of events by time t")
  points(PP,0*PP,type="p",pch=16)
  lines(PP,0:(length(PP)-1),type="h",lty=2)
}

plot.PP(day_ecart)

```

We observe that the cumulative number of events is displayed over time, showing an increasing trend. This growth appears to slightly accelerate as time progresses.

To do so, we begin by finding the optimal parameters for these functions using the maximum likelihood method.

```{r}
log_likelihood_exp <- function(params) {
  alpha <- params[1]
  beta <- params[2]
  lambda <- function(t) alpha * exp(beta * t)
  # Log-vraisemblance
  log_L <- sum(log(lambda(day_ecart))) - integrate(lambda, lower = 0, upper = max(day_ecart))$value
  return(-log_L)  # Minimiser l'opposé
}
params_exp <- optim(par = c(1, 0), log_likelihood_exp)$par

log_likelihood_log <- function(params) {
  alpha <- params[1]
  beta <- params[2]
  lambda <- function(t) alpha * log(1 + beta * t)
  log_L <- sum(log(lambda(day_ecart))) - integrate(lambda, lower = 0, upper = max(day_ecart))$value
  return(-log_L)
}
params_log <- optim(par = c(1, 1), log_likelihood_log)$par

log_likelihood_two_values <- function(params) {
  t0 <- params[1]
  lambda1 <- params[2]
  lambda2 <- params[3]
  lambda <- function(t) ifelse(t <= t0, lambda1, lambda2)
  log_L <- sum(log(sapply(day_ecart, lambda))) - integrate(lambda, lower = 0, upper = max(day_ecart))$value
  return(-log_L)
}
params_two_values <- optim(par = c(mean(day_ecart), 1, 1), log_likelihood_two_values)$par

log_likelihood_weibull <- function(params) {
  theta <- params[1]
  beta <- params[2]
  lambda <- function(t) (beta / theta) * (t / theta)^(beta - 1)
  log_L <- sum(log(lambda(day_ecart))) - integrate(lambda, lower = 0, upper = max(day_ecart))$value
  return(-log_L)
}
params_weibull <- optim(par = c(1, 1), log_likelihood_weibull)$par
```

Now that we have our functions well-defined with their various parameters, we calculate the AIC (Akaike Information Criterion) to determine which function best models our process.

```{r}
calculate_aic <- function(log_L, k) {
  return(2 * k - 2 * log_L)
}

# Calculer l'AIC pour chaque modèle
aic_exp <- calculate_aic(-log_likelihood_exp(params_exp), 2)
aic_log <- calculate_aic(-log_likelihood_log(params_log), 2)
aic_two_values <- calculate_aic(-log_likelihood_two_values(params_two_values), 3)
aic_weibull <- calculate_aic(-log_likelihood_weibull(params_weibull), 2)

aic_values <- c(aic_exp, aic_log, aic_two_values, aic_weibull)
names(aic_values) <- c("Exponential", "Logarithmic", "Two Values", "Weibull")
print(aic_values)
```

The AIC criterion must be minimized. Therefore, the two best models for our intensity function are the exponential function and the step function. This is consistent with the visual analysis conducted earlier.

Let us study the behavior of our intensity function using the Laplace and Weibull tests constructed in the first part. First, we extract the maximum time value from our data.

We now apply our tests to the data we have just retrieved:

```{r}
set.seed(18) 
T_max = day_max
alpha = 0.05
res = laplace_test(day_ecart,T_max,alpha)
print(paste("(Laplace) Rejection of H_0:", as.logical(res[3])))
print(paste("(Laplace) Statistical test observed:",res[1]))
print(paste("(Laplace) p-value:",res[2]))
```

In the context of the Laplace test, we reject $H_0$ at a 5% significance level. This suggests that the intensity function is increasing.

These results provide strong evidence supporting the hypothesis of an increasing intensity function.

```{r}
res = weibull_test(day_ecart,T_max,alpha)
print(paste("(Weibull) Rejection of H_0:", as.logical(res[3])))
print(paste("(Weibull) Statistical test observed:",res[1]))
print(paste("(Weibull) p-value:",res[2]))
```

For the Weibull test, we also reject $H_0$ at a 5% significance level with certainty. Therefore, the intensity function can be assumed to be increasing.

In conclusion, based on the Danish fires dataset, the results of the Laplace and Weibull tests consistently indicate that the intensity function is increasing. This suggests a growing frequency of large fire insurance claims over the observed period. These findings highlight the non-homogeneous nature of the underlying process and demonstrate the practical utility of the proposed tests in real-world applications.

## Conclusion

# A faire

#### *References* [1] Lee J Bain, Max Engelhardt, and FT Wright. "Tests for an increasing trend in the intensity of a Poisson process: A power study*". Journal of the American Statistical Association*, 80(390):419--422, 1985.
